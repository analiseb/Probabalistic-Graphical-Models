{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov networks\n",
    "\n",
    "In this notebook, we will have a taste of Markov networks (MNs). MNs have two building blocks too: an undirected  graph structure and a set of factors.\n",
    "\n",
    "We will use the classical **misconception example**, which has the following graph structure:\n",
    "\n",
    "<img src=\"images/misconception_graph.png\" style=\"width:200px\" />\n",
    "\n",
    "It is about a group of students (Alice, Bob, Charles and Debbie) that usually study together. The edges in the graph represent the established relationships. The lack of an edge between two nodes represents the lack of relationship.\n",
    "\n",
    "We have a factor for each edge (in this MN, maximal cliques):\n",
    "\n",
    "<img src=\"images/misconception_factors.png\" style=\"height:150px\" />\n",
    "\n",
    "Note that these factors are not probability distributions!\n",
    "\n",
    "Let us interpret these values. For example, B and C tend to agree with each other: having the same value (factor $\\phi_2$) is rewarded with a large value, whereas having different values is not. Agreement is also observed between A and B, but the strength is much smaller (we are not that sure about it). On the contrary, C and D usually disagree: note the high value assigned to combinations where the variables take different value ($\\phi_3$). \n",
    "\n",
    "\n",
    "The factorization of this MN is given by the product of the four pairwise factors and a normalizing partition function ($\\Theta$):\n",
    "\n",
    "$$p(A,B,C,D)=\\frac{1}{\\Theta}\\phi_{1}(A,B)\\phi_{2}(B,C)\\phi_{3}(C,D)\\phi_{4}(D,A)$$\n",
    "\n",
    "Thus, the factors can be represented again as matrices, but they do not sum up to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "phi_1 = np.array([[30, 5],\n",
    "                  [1, 10]])\n",
    "np.sum(phi_1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the basic operations are similar to the ones used with Bayesian networks. Let us see for example how we could implement a product of factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_2 = np.array([[100, 1],\n",
    "                  [1, 100]])\n",
    "\n",
    "prod = np.zeros(2**3)\n",
    "for a in [0,1]:\n",
    "    for b in [0,1]:\n",
    "        for c in [0,1]:\n",
    "            prod[np.ravel_multi_index([a,b,c],(2,2,2))] = phi_1[a,b]*phi_2[b,c]\n",
    "prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note always that we are not dealing with distributions anymore. Thus, if we want to infer a probability distribution from a MN we need to normalize. That is the role of the partition function ($\\Theta$): to transform this product of factors into a probability distribution useful for reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setting up our model with pgmpy\n",
    "\n",
    "We need to codify both elements: the undirected graph structure and the factors. \n",
    "\n",
    "First of all, we need to import the necessary functions from `pgmpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.factors.discrete import DiscreteFactor # this is the library's implementation of a factor\n",
    "from pgmpy.models import MarkovModel              # and the class for Markov network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the structure\n",
    "\n",
    "Now, we want to specify that we are constructing a Markov network with a set of undirected edges as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = MarkovModel()\n",
    "H.add_nodes_from(['Alice', 'Bob', 'Charles', 'Debbie'])\n",
    "H.add_edges_from([('Alice', 'Bob'),  ### YOUR CODE HERE ### , , ])\n",
    "\n",
    "print('Check if node `Alice´ is in the graph:', ('Alice' in H)) # check if node in graph\n",
    "print('Number of nodes in the graph:',len(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this simple code, we specify that there are four variables, each of them representing a student, and that there is an undirected edge connecting <i>Alice</i> and <i>Bob</i>, <i>Bob</i> and <i>Charles</i>, <i>Charles</i> and <i>Debbie</i>, and finally <i>Debbie</i> and <i>Alice</i>.\n",
    "\n",
    "\n",
    "### Set up the factor potentials\n",
    "\n",
    "Once the structure has been defined, we need to codify the factors. Firstly, the factor values assigned to all the posible combinations of values for variables <i>Alice</i> and <i>Bob</i> are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorAB = DiscreteFactor(['Alice', 'Bob'], cardinality=[2, 2],\n",
    "                          values=[30, 5, 1, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Following the same definition the other three factors are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorBC = DiscreteFactor(['Bob', 'Charles'], cardinality=[2, 2],\n",
    "                          values=[ #### YOUR CODE HERE #### ])\n",
    "\n",
    "factorCD = DiscreteFactor(['Charles', 'Debbie'], cardinality=[2, 2],\n",
    "                          values=[ #### YOUR CODE HERE #### ])\n",
    "\n",
    "factorDA = DiscreteFactor(['Debbie', 'Alice'], cardinality=[2, 2],\n",
    "                          values=[ #### YOUR CODE HERE #### ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once the potential factors are defined, we only have to include them into the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.add_factors(factorAB,factorBC,factorCD,factorDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us examine our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Is the model consistent?',H.check_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another important characteristic of Markov networks is the necessity of a partition function to build a joint probability distribution from the product of factors. This constant, represented in the above equation as $\\Theta$, is defined as the sum of the product for all the possible value combinations:\n",
    "$$\\Theta=\\sum_{a,b,c,d}\\phi_{AB}(A,B)\\phi_{BC}(B,C)\\phi_{CD}(C,D)\\phi_{DA}(D,A)$$\n",
    "\n",
    "Its value can be obtained as follow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Partition function value (constant):',H.get_partition_function())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our Markov network\n",
    "\n",
    "We have already built our model. It is time to use it!\n",
    "\n",
    "We can easily find all the local <b>independencies</b> in the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Set of conditional independencies in the MN:\\n',H.get_local_independencies())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This set is obtained, given each single random variable $X_i$, by looking for all the variables out of the <b>Markov Blanket</b> of $X_i$ as we know that those are independent from $X_i$ given its Mb:\n",
    "\n",
    "$$X_i\\perp \\{V \\backslash X_i\\backslash MB(X_i)\\} |MB(X_i)$$\n",
    "\n",
    "Thus, we might be interested in knowing the Markov Blanket of <i>Alice</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Markov Blanket of `Alice´:', list(H.markov_blanket('Alice')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Regarding the set of (conditional) independencies that are encoded by a Markov network and its equivalent, if possible, Bayesian network, we can transform the MN to a BN and observe the produced structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = H.to_bayesian_model()\n",
    "print('Set of directed edges in the BN:\\n',bn.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Set of conditional independencies in the BN:\\n',bn.get_independencies())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "and observe that this DAG $G$ is an I-map of the undirected graph $H$, $I(G)\\subseteq I(H)$, but does not represent the same set of conditional independence statements. **What is missing**? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr /> \n",
    "\n",
    "## Asking our Markov network\n",
    "\n",
    "Later in this course, we will know different approaches for inference in PGMs. However, let us consider the approach known as <i>Variable Elimination</i> to observe how we can perform different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "inference_with_H = VariableElimination(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can obtain the marginal probability distributions of different variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_alice = inference_with_H.query(variables = ['Alice'])\n",
    "print(prob_alice.normalize(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to normalize to obtain a probability distribution! What will we see if we don't?\n",
    "\n",
    "Similarly, the marginal probability distributions of <i>Bob</i> is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_bob = inference_with_H.query(variables = ['Bob'])\n",
    "print(prob_bob.normalize(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is interesting to observe that, although Alice and Bob are likely to agree according to factor potential $\\phi_1(A,B)$, the marginal probability distributions of both are not. This is a consequence of the rest of the relationships in the model and emphasizes the conceptual distance between potential factor and joint or conditional probability distributions.\n",
    "\n",
    "The most common use of inference techniques is for propagating the evidence gathered from certain observations. We can calculate the marginal probability <i>Alice</i> and <i>Charles</i> given that <i>Bob</i> is unaware of the misconception:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_Alice_Bob_unaware = inference_with_H.query(variables = ['Alice','Debbie'], \n",
    "                                              evidence = {'Bob':0})\n",
    "print(prob_Alice_Bob_unaware.normalize(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is also possible to codify evidence in more than one variable. For example, we can calculate the marginal probability <i>Alice</i> and <i>Debbie</i> given that <i>Bob</i> is unaware of the misconception but <i>Charles</i> is not, which brings more certainty to the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_Alice_Bob_disagree_Charles = inference_with_H.query(variables = ['Alice','Debbie'], \n",
    "                                              evidence = {'Bob':0,'Charles':1})\n",
    "print(prob_Alice_Bob_disagree_Charles.normalize(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We know from the previous discussion that $D\\perp B \\mid A,C$. We can experience that: let us imagine that we do know the answer of <i>Alice</i> and <i>Charles</i>. The probability of each possible answer of Debbie is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_Debbie_st_Alice_agree_Charles = inference_with_H.query(variables = ['Debbie'], \n",
    "                                                            evidence = {'Alice':1,'Charles':1})\n",
    "print(prob_Debbie_st_Alice_agree_Charles.normalize(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Does any new evidence about the answer of <i>Bob</i> contribute to update the distribution of Debbie's answer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bob is unaware\n",
    "prob_Debbie_st_Alice_agree_Charles_indBob0 = inference_with_H.query(variables = ['Debbie'], \n",
    "                                                                    evidence = {'Alice':1,'Charles':1,'Bob':0})\n",
    "print(prob_Debbie_st_Alice_agree_Charles_indBob0.normalize(False))\n",
    "\n",
    "# Bob is aware\n",
    "prob_Debbie_st_Alice_agree_Charles_indBob1 = inference_with_H.query(variables = ['Debbie'], \n",
    "                                                                    evidence = {'Alice':1,'Charles':1,'Bob':1})\n",
    "print(prob_Debbie_st_Alice_agree_Charles_indBob1.normalize(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "## Exercices\n",
    "\n",
    "- Which is the probability that Charles knows about the misconception given that Alice is unaware?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Which is the probability that Charles knows about the misconception given that Bob and Alice are unaware?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which is the probability that Charles knows about the misconception given that the rest of them (Alice, Bob, and Debbie) are unaware?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
